{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of RNN_text_generator_00.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m8xB0z_hYbG",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow 2.0 Text generator on Dante Alighieri's Divine Comedy\n",
        "\n",
        "Author: **Ivan Bongiorni**, [LinkedIn profile](https://www.linkedin.com/in/ivan-bongiorni-b8a583164/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncDUryHdqckI",
        "colab_type": "text"
      },
      "source": [
        "This Notebook contains a **text generator RNN** that was trained on the **Divina Commedia** (the *Divine Comedy*) by **Dante Alighieri**. This is a poem written at the beginning of the XII century. It's hard to explain what it represents for Italian culture: it's without any doubt the main pillar of our national literature, one of the building blocks of modern Italian language, and arguably the gratest poem ever. All modern representations of Hell, Purgatory and Heaven derive from this opera.\n",
        "\n",
        "It's structure is extremely interesting: each verse is composed of 11 syllables, and its rhymes follow an **A-B-A-B** structure. Lot of pattern to be learned! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8FnhxiWY_Y1",
        "colab_type": "code",
        "outputId": "d94fef1a-d3e6-4b2d-fa20-d22e5d64edaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import time\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Read file from Colab Notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.1.0-rc1\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5YiWU0daYkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_path = \" [...] /TF_2.0/NLP/text_generator/\"\n",
        "\n",
        "# Read the Divina Commedia\n",
        "with open(current_path + \"DivinaCommedia.txt\", 'r', encoding=\"utf8\") as file:\n",
        "    divina_commedia = file.read()\n",
        "\n",
        "# Replace rare characters\n",
        "divina_commedia = divina_commedia.replace(\"ä\", \"a\")\n",
        "divina_commedia = divina_commedia.replace(\"é\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"ë\", \"è\")\n",
        "divina_commedia = divina_commedia.replace(\"Ë\", \"E\")\n",
        "divina_commedia = divina_commedia.replace(\"ï\", \"i\")\n",
        "divina_commedia = divina_commedia.replace(\"Ï\", \"I\")\n",
        "divina_commedia = divina_commedia.replace(\"ó\", \"ò\")\n",
        "divina_commedia = divina_commedia.replace(\"ö\", \"o\")\n",
        "divina_commedia = divina_commedia.replace(\"ü\", \"u\")\n",
        "\n",
        "divina_commedia = divina_commedia.replace(\"(\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\")\", \"-\")\n",
        "divina_commedia = divina_commedia.replace(\"[\", \"\")\n",
        "divina_commedia = divina_commedia.replace(\"]\", \"\")\n",
        "\n",
        "divina_commedia = re.sub(r'[0-9]+', '', divina_commedia)\n",
        "divina_commedia = divina_commedia.replace(\" \\n\", \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILphRXIXaYrP",
        "colab_type": "code",
        "outputId": "c102b647-0d95-45cd-c162-49d07140d96e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check lenght of text\n",
        "print(len(divina_commedia))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "551697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZw3-Joyhg5l",
        "colab_type": "text"
      },
      "source": [
        "I will now extract the set of unique characters, and create a dictionary for vectorization of text. In order to feed the text into a Neural Network, I must turn each character into a number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwvjeLGWAVE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store unique characters into a dict with numerical encoding\n",
        "unique_chars = list(set(divina_commedia))\n",
        "unique_chars.sort()  # to make sure you get the same encoding at each run\n",
        "\n",
        "# Store them in a dict, associated with a numerical index\n",
        "char2idx = { char[1]: char[0] for char in enumerate(unique_chars) }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkXOJ3LGAVCF",
        "colab_type": "code",
        "outputId": "c8b49300-9d1e-49e4-a7e2-0b0f70be5ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(char2idx))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sieJxDhAU_V",
        "colab_type": "code",
        "outputId": "0463ef4c-3ff1-466e-d0cd-5a5e5cfd52c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "char2idx"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " \"'\": 4,\n",
              " ',': 5,\n",
              " '-': 6,\n",
              " '.': 7,\n",
              " ':': 8,\n",
              " ';': 9,\n",
              " '?': 10,\n",
              " 'A': 11,\n",
              " 'B': 12,\n",
              " 'C': 13,\n",
              " 'D': 14,\n",
              " 'E': 15,\n",
              " 'F': 16,\n",
              " 'G': 17,\n",
              " 'H': 18,\n",
              " 'I': 19,\n",
              " 'L': 20,\n",
              " 'M': 21,\n",
              " 'N': 22,\n",
              " 'O': 23,\n",
              " 'P': 24,\n",
              " 'Q': 25,\n",
              " 'R': 26,\n",
              " 'S': 27,\n",
              " 'T': 28,\n",
              " 'U': 29,\n",
              " 'V': 30,\n",
              " 'X': 31,\n",
              " 'Z': 32,\n",
              " 'a': 33,\n",
              " 'b': 34,\n",
              " 'c': 35,\n",
              " 'd': 36,\n",
              " 'e': 37,\n",
              " 'f': 38,\n",
              " 'g': 39,\n",
              " 'h': 40,\n",
              " 'i': 41,\n",
              " 'j': 42,\n",
              " 'l': 43,\n",
              " 'm': 44,\n",
              " 'n': 45,\n",
              " 'o': 46,\n",
              " 'p': 47,\n",
              " 'q': 48,\n",
              " 'r': 49,\n",
              " 's': 50,\n",
              " 't': 51,\n",
              " 'u': 52,\n",
              " 'v': 53,\n",
              " 'x': 54,\n",
              " 'y': 55,\n",
              " 'z': 56,\n",
              " 'È': 57,\n",
              " 'à': 58,\n",
              " 'è': 59,\n",
              " 'ì': 60,\n",
              " 'ò': 61,\n",
              " 'ù': 62}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2H5G86HhyvE",
        "colab_type": "text"
      },
      "source": [
        "Once I have a dictionary that maps each characted with its respective numerical index, I can process the whole corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk_BqFK4AU9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numerical_encoding(text, char_dict):\n",
        "    \"\"\" Text to list of chars, to np.array of numerical idx \"\"\"\n",
        "    chars_list = [ char for char in text ]\n",
        "    chars_list = [ char_dict[char] for char in chars_list ]\n",
        "    chars_list = np.array(chars_list)\n",
        "    return chars_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7JpYN9LAU7U",
        "colab_type": "code",
        "outputId": "78a68da8-af24-471c-c867-b87d1590cb4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Let's see what the first line will look like\n",
        "print(\"{}\".format(divina_commedia[276:311]))\n",
        "print(\"\\nbecomes:\")\n",
        "print(numerical_encoding(divina_commedia[276:311], char2idx))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nel mezzo del cammin di nostra vita\n",
            "\n",
            "becomes:\n",
            "[22 37 43  1 44 37 56 56 46  1 36 37 43  1 35 33 44 44 41 45  1 36 41  1\n",
            " 45 46 50 51 49 33  1 53 41 51 33]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqALeTUmnl0X",
        "colab_type": "text"
      },
      "source": [
        "## RNN dataprep\n",
        "\n",
        "I need to generate a set of stacked input sequences. My goal is to train a Neural Network to find a mapping between an input sequence and an output sequence of equal length, in which each character is shifted left of one position.\n",
        "\n",
        "For example, the first verse:\n",
        "\n",
        "> Nel mezzo del cammin di nostra vita\n",
        "\n",
        "would be translated in a train sequence as:\n",
        "\n",
        "`Nel mezzo del cammin di nostra vit`\n",
        "\n",
        "be associated with the target sequence:\n",
        "\n",
        "`el mezzo del cammin di nostra vita`\n",
        "\n",
        "The following function is a preparatory step for that. More generally, given a sequence:\n",
        "\n",
        "```\n",
        "A B C D E F G H I\n",
        "```\n",
        "\n",
        "and assuming input sequences of length 5, it will generate a matrix like:\n",
        "\n",
        "```\n",
        "A B C D E\n",
        "B C D E F\n",
        "C D E F G\n",
        "D E F G H\n",
        "E F G H I\n",
        "```\n",
        "\n",
        "I will save that matrix as it is in .csv format, to use it to train the Language Generator later.\n",
        "The split between train and target sets will be as:\n",
        "\n",
        "```\n",
        " Train:           Target:\n",
        "                 \n",
        "A B C D E        B C D E F\n",
        "B C D E F        C D E F G\n",
        "C D E F G        D E F G H\n",
        "D E F G H        E F G H I\n",
        "                 \n",
        "```\n",
        "\n",
        "Train and target sets are fundamentally the same matrix, with the train having the last row removed, and the target set having the first removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWcZzOJdG6X9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply it on the whole Comedy\n",
        "encoded_text = numerical_encoding(divina_commedia, char2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t41gYByxAU4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_text_matrix(sequence, len_input):\n",
        "    \n",
        "    # create empty matrix\n",
        "    X = np.empty((len(sequence)-len_input, len_input))\n",
        "    \n",
        "    # fill each row/time window from input sequence\n",
        "    for i in range(X.shape[0]):\n",
        "        X[i,:] = sequence[i : i+len_input]\n",
        "        \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eazAAQiAk0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_matrix = get_text_matrix(encoded_text, 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KonviQjQAk40",
        "colab_type": "code",
        "outputId": "ba43429c-27d1-4c79-b629-9f17c02b5214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(text_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(551597, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaVngDG7AkyU",
        "colab_type": "code",
        "outputId": "55c0d743-b655-4b5c-a83c-bd39857b3d05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "print(\"100th train sequence:\\n\")\n",
        "print(text_matrix[ 100, : ])\n",
        "print(\"\\n\\n100th target sequence:\\n\")\n",
        "print(text_matrix[ 101, : ])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100th train sequence:\n",
            "\n",
            "[45. 37.  1. 37.  1. 47. 52. 45. 41. 44. 37. 45. 51. 41.  1. 36. 37.  4.\n",
            "  1. 53. 41. 56. 41.  1. 37.  1. 36. 37.  4.  1. 44. 37. 49. 41. 51. 41.\n",
            "  1. 37.  1. 47. 49. 37. 44. 41.  1. 36. 37.  1. 43. 37.  1. 53. 41. 49.\n",
            " 51. 62.  7.  1. 13. 46. 44. 41. 45. 35. 41. 33.  1. 41. 43.  1. 35. 33.\n",
            " 45. 51. 46.  1. 47. 49. 41. 44. 46.  1. 36. 37.  1. 43. 33.  1. 47. 49.\n",
            " 41. 44. 33.  1. 47. 33. 49. 51. 37.  1.]\n",
            "\n",
            "\n",
            "100th target sequence:\n",
            "\n",
            "[37.  1. 37.  1. 47. 52. 45. 41. 44. 37. 45. 51. 41.  1. 36. 37.  4.  1.\n",
            " 53. 41. 56. 41.  1. 37.  1. 36. 37.  4.  1. 44. 37. 49. 41. 51. 41.  1.\n",
            " 37.  1. 47. 49. 37. 44. 41.  1. 36. 37.  1. 43. 37.  1. 53. 41. 49. 51.\n",
            " 62.  7.  1. 13. 46. 44. 41. 45. 35. 41. 33.  1. 41. 43.  1. 35. 33. 45.\n",
            " 51. 46.  1. 47. 49. 41. 44. 46.  1. 36. 37.  1. 43. 33.  1. 47. 49. 41.\n",
            " 44. 33.  1. 47. 33. 49. 51. 37.  1. 43.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPjMSZOzh_60",
        "colab_type": "text"
      },
      "source": [
        "# Architecture\n",
        "\n",
        "At this point, I can specify the RNN architecture with all its hyperparameters. An `Embedding()` layer will first learn a representation of each character; the sequence of chracters embedding will then be fed into an `LSTM()` layer, that will extract information from their sequence; `Dense()` layers at the end will produce the next character prediction.\n",
        "\n",
        "The Network is structured to be fed with batches of data of fixed size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFZfbimYAkvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# size of vocabulary\n",
        "vocab_size = len(char2idx)\n",
        "\n",
        "# size of mini batches during training\n",
        "batch_size = 100\n",
        "\n",
        "# size of training subset at each epoch\n",
        "subset_size = batch_size * 100\n",
        "\n",
        "# vector size of char embeddings\n",
        "embedding_size = 250\n",
        "\n",
        "len_input = 1000   # 200\n",
        "\n",
        "hidden_size = 250  # for Dense() layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1JqO7rhAktC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.activations import elu, relu, softmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKysyBfpAtUS",
        "colab_type": "code",
        "outputId": "8b81d5fb-d1ea-4341-a214-c7cb4bac0a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "seq2seq = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(batch_size, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "seq2seq.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (100, None, 250)          15750     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (100, None, 1000)         5004000   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (100, None, 250)          250250    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (100, None, 63)           15813     \n",
            "=================================================================\n",
            "Total params: 5,285,813\n",
            "Trainable params: 5,285,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9Tldq4qAtXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 100\n",
        "\n",
        "learning_rate = 0.0001\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUgGeC5ZAtbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is an Autograph function\n",
        "# its decorator makes it a TF op - i.e. much faster\n",
        "@tf.function\n",
        "def train_on_batch(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        current_loss = tf.reduce_mean(\n",
        "            tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                y, seq2seq(x), from_logits = True))\n",
        "    gradients = tape.gradient(current_loss, seq2seq.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, seq2seq.trainable_variables))\n",
        "    return current_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-x5RxKrAtnR",
        "colab_type": "code",
        "outputId": "3ea20420-858b-4953-a056-2fbaf451edd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "loss_history = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    \n",
        "    # Take subsets of train and target\n",
        "    sample = np.random.randint(0, text_matrix.shape[0]-1, subset_size)\n",
        "    sample_train = text_matrix[ sample , : ]\n",
        "    sample_target = text_matrix[ sample+1 , : ]\n",
        "    \n",
        "    for iteration in range(sample_train.shape[0] // batch_size):\n",
        "        start = iteration * batch_size\n",
        "        x = sample_train[ start:start+batch_size , : ]\n",
        "        y = sample_target[ start:start+batch_size , : ]\n",
        "\n",
        "        current_loss = train_on_batch(x, y)\n",
        "        loss_history.append(current_loss)\n",
        "    \n",
        "    print(\"{}.  \\t  Loss: {}  \\t  Time: {}ss\".format(\n",
        "        epoch+1, current_loss.numpy(), round(time.time()-start, 2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.  \t  Loss: 3.111814022064209  \t  Time: 1578747534.28ss\n",
            "2.  \t  Loss: 2.937624216079712  \t  Time: 1578747541.8ss\n",
            "3.  \t  Loss: 2.6059911251068115  \t  Time: 1578747549.32ss\n",
            "4.  \t  Loss: 2.442699670791626  \t  Time: 1578747556.83ss\n",
            "5.  \t  Loss: 2.336099147796631  \t  Time: 1578747564.33ss\n",
            "6.  \t  Loss: 2.273846387863159  \t  Time: 1578747571.85ss\n",
            "7.  \t  Loss: 2.2103261947631836  \t  Time: 1578747579.38ss\n",
            "8.  \t  Loss: 2.1668601036071777  \t  Time: 1578747586.89ss\n",
            "9.  \t  Loss: 2.1551244258880615  \t  Time: 1578747594.4ss\n",
            "10.  \t  Loss: 2.09932017326355  \t  Time: 1578747601.94ss\n",
            "11.  \t  Loss: 2.076141119003296  \t  Time: 1578747609.51ss\n",
            "12.  \t  Loss: 2.0864453315734863  \t  Time: 1578747617.05ss\n",
            "13.  \t  Loss: 2.0433664321899414  \t  Time: 1578747624.58ss\n",
            "14.  \t  Loss: 2.0631515979766846  \t  Time: 1578747632.12ss\n",
            "15.  \t  Loss: 2.000927448272705  \t  Time: 1578747639.65ss\n",
            "16.  \t  Loss: 1.9632011651992798  \t  Time: 1578747647.17ss\n",
            "17.  \t  Loss: 1.976094365119934  \t  Time: 1578747654.7ss\n",
            "18.  \t  Loss: 1.9480818510055542  \t  Time: 1578747662.25ss\n",
            "19.  \t  Loss: 1.9344466924667358  \t  Time: 1578747669.77ss\n",
            "20.  \t  Loss: 1.912446141242981  \t  Time: 1578747677.3ss\n",
            "21.  \t  Loss: 1.9057296514511108  \t  Time: 1578747684.83ss\n",
            "22.  \t  Loss: 1.8813519477844238  \t  Time: 1578747692.37ss\n",
            "23.  \t  Loss: 1.8712382316589355  \t  Time: 1578747699.9ss\n",
            "24.  \t  Loss: 1.8468605279922485  \t  Time: 1578747707.42ss\n",
            "25.  \t  Loss: 1.8348835706710815  \t  Time: 1578747714.95ss\n",
            "26.  \t  Loss: 1.8258464336395264  \t  Time: 1578747722.49ss\n",
            "27.  \t  Loss: 1.8445384502410889  \t  Time: 1578747730.02ss\n",
            "28.  \t  Loss: 1.8060425519943237  \t  Time: 1578747737.56ss\n",
            "29.  \t  Loss: 1.8173717260360718  \t  Time: 1578747745.09ss\n",
            "30.  \t  Loss: 1.800804853439331  \t  Time: 1578747752.63ss\n",
            "31.  \t  Loss: 1.8105242252349854  \t  Time: 1578747760.16ss\n",
            "32.  \t  Loss: 1.7656382322311401  \t  Time: 1578747767.68ss\n",
            "33.  \t  Loss: 1.760419487953186  \t  Time: 1578747775.22ss\n",
            "34.  \t  Loss: 1.7732205390930176  \t  Time: 1578747782.77ss\n",
            "35.  \t  Loss: 1.7541840076446533  \t  Time: 1578747790.32ss\n",
            "36.  \t  Loss: 1.7464262247085571  \t  Time: 1578747797.85ss\n",
            "37.  \t  Loss: 1.7322771549224854  \t  Time: 1578747805.36ss\n",
            "38.  \t  Loss: 1.7245210409164429  \t  Time: 1578747812.9ss\n",
            "39.  \t  Loss: 1.70206880569458  \t  Time: 1578747820.42ss\n",
            "40.  \t  Loss: 1.7349196672439575  \t  Time: 1578747827.93ss\n",
            "41.  \t  Loss: 1.722869873046875  \t  Time: 1578747835.46ss\n",
            "42.  \t  Loss: 1.7120267152786255  \t  Time: 1578747842.99ss\n",
            "43.  \t  Loss: 1.7064476013183594  \t  Time: 1578747850.5ss\n",
            "44.  \t  Loss: 1.6813710927963257  \t  Time: 1578747858.02ss\n",
            "45.  \t  Loss: 1.6885722875595093  \t  Time: 1578747865.56ss\n",
            "46.  \t  Loss: 1.6411552429199219  \t  Time: 1578747873.1ss\n",
            "47.  \t  Loss: 1.6170759201049805  \t  Time: 1578747880.62ss\n",
            "48.  \t  Loss: 1.6479367017745972  \t  Time: 1578747888.16ss\n",
            "49.  \t  Loss: 1.6297876834869385  \t  Time: 1578747895.69ss\n",
            "50.  \t  Loss: 1.6132469177246094  \t  Time: 1578747903.22ss\n",
            "51.  \t  Loss: 1.6595921516418457  \t  Time: 1578747910.74ss\n",
            "52.  \t  Loss: 1.6084409952163696  \t  Time: 1578747918.28ss\n",
            "53.  \t  Loss: 1.6309716701507568  \t  Time: 1578747925.8ss\n",
            "54.  \t  Loss: 1.5932525396347046  \t  Time: 1578747933.33ss\n",
            "55.  \t  Loss: 1.5777056217193604  \t  Time: 1578747940.85ss\n",
            "56.  \t  Loss: 1.5442595481872559  \t  Time: 1578747948.4ss\n",
            "57.  \t  Loss: 1.5655134916305542  \t  Time: 1578747955.93ss\n",
            "58.  \t  Loss: 1.5697476863861084  \t  Time: 1578747963.45ss\n",
            "59.  \t  Loss: 1.5342950820922852  \t  Time: 1578747971.03ss\n",
            "60.  \t  Loss: 1.557044506072998  \t  Time: 1578747978.57ss\n",
            "61.  \t  Loss: 1.542036771774292  \t  Time: 1578747986.11ss\n",
            "62.  \t  Loss: 1.5377529859542847  \t  Time: 1578747993.64ss\n",
            "63.  \t  Loss: 1.5515509843826294  \t  Time: 1578748001.19ss\n",
            "64.  \t  Loss: 1.511242389678955  \t  Time: 1578748008.75ss\n",
            "65.  \t  Loss: 1.5322035551071167  \t  Time: 1578748016.29ss\n",
            "66.  \t  Loss: 1.5138529539108276  \t  Time: 1578748023.83ss\n",
            "67.  \t  Loss: 1.5061880350112915  \t  Time: 1578748031.38ss\n",
            "68.  \t  Loss: 1.5033316612243652  \t  Time: 1578748038.93ss\n",
            "69.  \t  Loss: 1.4956737756729126  \t  Time: 1578748046.45ss\n",
            "70.  \t  Loss: 1.4686695337295532  \t  Time: 1578748053.98ss\n",
            "71.  \t  Loss: 1.4702526330947876  \t  Time: 1578748061.51ss\n",
            "72.  \t  Loss: 1.4866955280303955  \t  Time: 1578748069.04ss\n",
            "73.  \t  Loss: 1.456284523010254  \t  Time: 1578748076.56ss\n",
            "74.  \t  Loss: 1.473556637763977  \t  Time: 1578748084.16ss\n",
            "75.  \t  Loss: 1.465773105621338  \t  Time: 1578748091.71ss\n",
            "76.  \t  Loss: 1.4591286182403564  \t  Time: 1578748099.23ss\n",
            "77.  \t  Loss: 1.4564791917800903  \t  Time: 1578748106.76ss\n",
            "78.  \t  Loss: 1.4250074625015259  \t  Time: 1578748114.27ss\n",
            "79.  \t  Loss: 1.450943112373352  \t  Time: 1578748121.79ss\n",
            "80.  \t  Loss: 1.4128527641296387  \t  Time: 1578748129.3ss\n",
            "81.  \t  Loss: 1.4299410581588745  \t  Time: 1578748136.82ss\n",
            "82.  \t  Loss: 1.388879418373108  \t  Time: 1578748144.34ss\n",
            "83.  \t  Loss: 1.4015023708343506  \t  Time: 1578748151.88ss\n",
            "84.  \t  Loss: 1.3744137287139893  \t  Time: 1578748159.41ss\n",
            "85.  \t  Loss: 1.4342697858810425  \t  Time: 1578748166.92ss\n",
            "86.  \t  Loss: 1.4211714267730713  \t  Time: 1578748174.44ss\n",
            "87.  \t  Loss: 1.3976521492004395  \t  Time: 1578748181.98ss\n",
            "88.  \t  Loss: 1.4058758020401  \t  Time: 1578748189.49ss\n",
            "89.  \t  Loss: 1.3758971691131592  \t  Time: 1578748197.04ss\n",
            "90.  \t  Loss: 1.4143704175949097  \t  Time: 1578748204.58ss\n",
            "91.  \t  Loss: 1.3850281238555908  \t  Time: 1578748212.1ss\n",
            "92.  \t  Loss: 1.3966583013534546  \t  Time: 1578748219.6ss\n",
            "93.  \t  Loss: 1.355513095855713  \t  Time: 1578748227.11ss\n",
            "94.  \t  Loss: 1.3580726385116577  \t  Time: 1578748234.64ss\n",
            "95.  \t  Loss: 1.3572818040847778  \t  Time: 1578748242.14ss\n",
            "96.  \t  Loss: 1.3629984855651855  \t  Time: 1578748249.66ss\n",
            "97.  \t  Loss: 1.348604679107666  \t  Time: 1578748257.17ss\n",
            "98.  \t  Loss: 1.3492231369018555  \t  Time: 1578748264.68ss\n",
            "99.  \t  Loss: 1.305910587310791  \t  Time: 1578748272.2ss\n",
            "100.  \t  Loss: 1.3179041147232056  \t  Time: 1578748279.72ss\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6uAhmV2Atth",
        "colab_type": "code",
        "outputId": "ebbdc78b-820e-4bdb-c845-0868894c88ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plt.plot(loss_history)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCsDapUTBBrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq2seq.save(current_path + \"models/text_generator_RNN_00.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrMdonRckAWB",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation\n",
        "\n",
        "At this point, let's check how the model generates text. In order to do it, I must make some changes to my RNN architecture above.\n",
        "\n",
        "First, I must change the fixed batch size. After training, I want to feed just one sentence into my Network to make it continue the character sequence. I will feed a string into the model, make it predict the next character, update the input sequence, and repeat the process until a long generated text is obtained. Because of this, the succession of input sequences is now different from training session, in which portions of text were sampled randomly. I now have to set `stateufl = True` in the `LSTM()` layer, so that each LSTM cell will keep in memory the internal state from the previous sequence. With this I hope the model will better remember sequential information while generating text.\n",
        "\n",
        "I will instantiate a new `generator` RNN with these new features, and transfer the trained weights of my `RNN` into it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69aV7FGVAtxY",
        "colab_type": "code",
        "outputId": "af5cc318-56ac-4434-843d-56ab44e38a25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "generator = Sequential([\n",
        "    Embedding(vocab_size, embedding_size,\n",
        "              batch_input_shape=(1, None)),\n",
        "    \n",
        "    LSTM(len_input, return_sequences = True, stateful=True),\n",
        "    \n",
        "    Dense(hidden_size, activation = relu), \n",
        "    \n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "generator.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 250)            15750     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (1, None, 1000)           5004000   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (1, None, 250)            250250    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (1, None, 63)             15813     \n",
            "=================================================================\n",
            "Total params: 5,285,813\n",
            "Trainable params: 5,285,813\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whB1azhVAtrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import trained weights from RNN to generator\n",
        "generator.set_weights(seq2seq.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE2hYSqAAtkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(start_string, num_generate = 1000, temperature = 1.0):\n",
        "    \n",
        "    # Vectorize input string\n",
        "    input_eval = [char2idx[s] for s in start_string]  \n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    \n",
        "    text_generated = [] # List to append predicted chars \n",
        "    \n",
        "    idx2char = { v: k for k, v in char2idx.items() }  # invert char-index mapping\n",
        "    \n",
        "    generator.reset_states()\n",
        "    \n",
        "    for i in range(num_generate):\n",
        "        predictions = generator(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # sample next char based on distribution and temperature\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        \n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        \n",
        "    return (start_string + ''.join(text_generated))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hBCF5YrqBtG",
        "colab_type": "text"
      },
      "source": [
        "(This function is based on [this tutorial](https://www.tensorflow.org/tutorials/text/text_generation).)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_NWeo1fAtiC",
        "colab_type": "code",
        "outputId": "155f105a-a196-4453-f11a-685a0eb003b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Let's feed the first lines:\n",
        "start_string = \"\"\"\n",
        "Nel mezzo del cammin di nostra vita\n",
        "mi ritrovai per una selva oscura,\n",
        "chè la diritta via era smarrita.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for t in [0.1, 0.5, 1.0, 1.5, 2]:\n",
        "    print(\"####### TEXT GENERATION - temperature = {}\\n\".format(t))\n",
        "    print(generate_text(start_string = start_string, num_generate = 1000, temperature = 1.0))\n",
        "    print(\"\\n\\n\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "####### TEXT GENERATION - temperature = 0.1\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Ma come scopinsi, ma nostra parve:\n",
            "da cove già, leggendo e suo cammino\n",
            "nel fambo che di sè Virgilla è vuope!\n",
            "\n",
            "Io quando per questo cader levando,\n",
            "ivi un grazio suover ben fam ditti,\n",
            "sì come sarei il nome dire; che nullo\n",
            "sen la man del templasse lo spondo\n",
            "che non veder, sì piacer sù tembiante.\n",
            "\n",
            "Masiziava là si converse a fetto:\n",
            "sì com'om com'amor siamentia mai;\n",
            "non ti veggia con la somalina essa.\n",
            "\n",
            "Ondo bella è baladoso e 'Sbecchi\n",
            "anime annava il dabbrosco sol trova,\n",
            "per lo non saede inde mi divisia,\n",
            "e fantue punge lue verso mi racco,\n",
            "\n",
            "dinanzi a era allor si volme amori,\n",
            "la rimanebbe col sì al pene detto,\n",
            "ch'apparò convien che tuo veduti fanno.\n",
            "\n",
            "E contrui al figliuol divina e bestie\n",
            "li ostre spiriti che 'l verco secca\n",
            "proverene a ciò che 'ntrava e per ristei,\n",
            "prescosse scuole, per le genti schieri,\n",
            "se per lo punto quell'anima truva,\n",
            "nabitando la ventita da la trida,\n",
            "\n",
            "fordan di Cami almor di gritta in susa;\n",
            "ma da cagione andan, ch'io mi fede,\n",
            "mi passi a tanto mille etternale\n",
            "e disfansi a \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 0.5\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Già vede la troppo attende la copa\n",
            "d'un graddi ovi parenza sè ben forsi,\n",
            "del bulgo foco insimile la vitta,\n",
            "in come lui in sè grappo il suo bocca,\n",
            "e ch'io non sapre ancor si vide bene.\n",
            "\n",
            "Possa nostra vita nico de la fronte\n",
            "vedesi talle d'alcun girbo giusso;\n",
            "e d'elle erraldo le fuor del suo fattore,\n",
            "dal frasco vero e col mentì del sonno.\n",
            "\n",
            "Era piegade e dal dibrino parlato,\n",
            "mostrandosi la lascia, che non purga\n",
            "perduta sovien questo impeduto del,\n",
            "quel con le porte quando ti bai profondo,\n",
            "reggegnand' io a l'antera efferto.\n",
            "\n",
            "Le bosciutelle, fa ch'al potto lor frente,\n",
            "e nel congiummer o gente confottaggio\n",
            "è già d'un diro, altra 'l disio e ne a mino,\n",
            "nel bal dolcia trovar le rive e divessi.\n",
            "\n",
            "Quale gente buona oranza ne la corda,\n",
            "e quieta noi poscia, poi come fatto\n",
            "e buon fer di Ruta Spiegè che 'l confia\n",
            "oè quinci vostro 'l qual si diganna notto.\n",
            "\n",
            "E 'l sangue in alto più le manguette,\n",
            "cominciò 'l buon namor che 'l dolce via\n",
            "ben va il paregio e guidad in campo\".\n",
            "\n",
            "\"Se venea, crudel di sè tuttuitai\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 1.0\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Quando sol esso li ore, donna 'l conno\n",
            "da Piè là sùbigliando la cincia\n",
            "de la mio abille; e però che cossi\n",
            "tolmeggiava il suo disiere avviva\".\n",
            "\n",
            "Or sapiti alcun che marai fenimeto\n",
            "pur guardato, e dal letto più era essito\n",
            "canto con vid'io chiedi quelle specchi!\n",
            "\n",
            "Vassi la qual va, sentre che scheam predi,\n",
            "se non tramai più tornar li fossemo.\n",
            "\n",
            "Le muterale tua, che porri non piedi\",\n",
            "rispuose\", ritorno ad elli a l'altro rama,\n",
            "con quando in dietro è, a nostra paresta,\n",
            "per l'agura gente che tre dilemi schiuma.\n",
            "\n",
            "Non purò m'amor de la pesa prima,\n",
            "se tanto faccia si distele al calo,\n",
            "e ben rei santa questo roppe suove.\n",
            "\n",
            "Vederabella che la sèa basalla\n",
            "non penstiri aversi lui del vostro rote,\n",
            "non me, se nassun, ma perchè t'apposta\n",
            "col dresta in su la pestana forma uscia,\n",
            "bolla la etasian di Fiorentine;\n",
            "\n",
            "in reme, richiera quel semer l'altro,\n",
            "venerusammente del via taceve\n",
            "verto è a bel celest'io era approsso,\n",
            "a la riva pur mei cario funo.\n",
            "\n",
            "Parean le caglia non che guardisme\n",
            "quella dentro volumente, acc\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 1.5\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Quell'anima che si men difetto regno;\n",
            "ma disse: \"Mate, se quel cinto del trito,\n",
            "che la vostra vive di terra racque\n",
            "\n",
            "e rapofare bella, chè vogna goglia\n",
            "di grado suo parver dentro altrui piume;\n",
            "e tanto che 'n che per la berdesa volte,\n",
            "del tratto giorno uscor tir di quel move.\n",
            "\n",
            "Io minò e duol; esse più dimendosi\n",
            "le nove giorno a che il piacer sevve,\n",
            "sì benò la torria del lume redo\n",
            "che la sua colpa che non fugge plende.\n",
            "\n",
            "Poi indi 'l visio manìzion che fosse:\n",
            "e ne la trebba a la Terra crista,\n",
            "se nanqui dirno da le tu pur parole,\n",
            "patricersi s'avea lasciato. \"Sempienza\n",
            "per lo voler, mo suose, per mia tarri\",\n",
            "disse 'l vital più che già nel cielo.\n",
            "\n",
            "Non mi disse: ond' me: \"Non mi parvenza;\n",
            "ma giunto mi ferma in essa e santo,\n",
            "\n",
            "alquanto per le stume si favella\n",
            "ne di pianger non con la costa schella\n",
            "mi parea stan dolce mai sù hai nacque.\n",
            "\n",
            "Quale io peccarti, solvendo in suo spiro\n",
            "e liguamente a conteniro a fiore\n",
            "in eran torta il suo belli è dicea\".\n",
            "Piò di contorme a sogliero i riguari?\";\n",
            "E\n",
            "\"Lo domo\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "####### TEXT GENERATION - temperature = 2\n",
            "\n",
            "\n",
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "chè la diritta via era smarrita.\n",
            "\n",
            "Io avvelle omai e 'l foco durano,\n",
            "ma non ha infilmata di collosa guarda,\n",
            "che l'ondava porzia consista e questa,\n",
            "fuor de lo ruder, sì tanta più menta\n",
            "e partinge; e io, nuè farmi più.\n",
            "\n",
            "Sanz'ami ne la fecilata in Costa,\n",
            "sì che dal ora il carlon l'affinse\n",
            "per non capreggemento i di gulide.\n",
            "\n",
            "Di fuor ben sodi, imai là dove 'l deco\n",
            "mi feri uominati a me la lunga rota,\n",
            "e non forse calcitti ancosa e verde\n",
            "quello a che già tornar partire e deste.\n",
            "\n",
            "Questi fuori aspeti, v'omi dinanzi,\n",
            "tal che la ponta loro io pruno fanno\n",
            "di cenestizia di Cabolta, batte\n",
            "di geme folgo; e puria tanto furito\n",
            "col figlio ove l'uccaso: perchè 'l palvano\n",
            "che passer lì vilente il penfiglio.\n",
            "\n",
            "Voi non pur con l'anime tua lata,\n",
            "potender la linga pria per li 'nsieme\n",
            "l'acquasir torta, e parebbero imbonti\n",
            "andor sen gravi for semorino punite;\n",
            "\n",
            "ma per lo seguite into le nè il piacque,\n",
            "ma lentador ch'è perver certi versi\n",
            "grava 'l potenza in giù, mal non si corde,\n",
            "e Aramina e di presti per dirci' lo schio.\n",
            "\n",
            "\n",
            "Va; 'Ma ben com'io tenea\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8KlRGTqmGBS",
        "colab_type": "text"
      },
      "source": [
        "The best generation is, IMHO, the one with `temperature = 1.5`. The sentences of course do not make sense, but it's amazing that such a simple model could achieve similar results, and generate absolutely Dante-esque text with just ~40 minutes of GPU training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtB2gvhimUEs",
        "colab_type": "text"
      },
      "source": [
        "Many things could be done at this point:\n",
        "\n",
        "\n",
        "\n",
        "*   Try fancier architectures, such as seq2seq. (I must say though that stacked RNNs didn't provide better results during prototyping.)\n",
        "*   Try Attention models.\n",
        "*   Longer training.\n",
        "*   Adversarial training.\n",
        "\n",
        "I'll try a lot of these techniques, alone and combined. My goal is to make a model that can learn the amazing structure of syllables and rhymes of the whole Comedy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbXuy2OopudL",
        "colab_type": "text"
      },
      "source": [
        "### Sources\n",
        "\n",
        "The main source I used to learn to implement an RNN text generator is [this tutorial on the official TensorFlow website](https://www.tensorflow.org/tutorials/text/text_generation)."
      ]
    }
  ]
}
